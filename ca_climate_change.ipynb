{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Install and import Library<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "%pip install pandas openpyxl\n",
    "%pip install seaborn\n",
    "%pip install shapely\n",
    "%pip install geodatasets\n",
    "%pip install geopy\n",
    "%pip install folium\n",
    "%pip install ipywidgets\n",
    "%pip install IPython\n",
    "%pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import openpyxl\n",
    "import zipfile\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "import geodatasets\n",
    "from geopy.geocoders import Nominatim\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import MarkerCluster\n",
    "from folium.plugins import HeatMap\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from flask import Flask, render_template_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> ETL - Load, transform and save data to SQL database <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upzip geojson file \n",
    "zip_file_path = 'Resources/California_Fire_Perimeters.geojson.zip'\n",
    "\n",
    "# Open the zip file in read mode\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Extract all the contents of the zip file\n",
    "    zip_ref.extractall('Resources')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wildfire Data\n",
    "# Name of the geojson file\n",
    "file_js = Path('Resources/California_Fire_Perimeters.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Wildfire GeoJSON Data\n",
    "wildfire_gdf = gpd.read_file(file_js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows to understand the structure\n",
    "print(wildfire_gdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract column names\n",
    "column_names = wildfire_gdf.columns\n",
    "print(\"Column Names:\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the minimum and maximum year in the dataset\n",
    "min_year = wildfire_gdf['YEAR_'].min()\n",
    "max_year = wildfire_gdf['YEAR_'].max()\n",
    "\n",
    "print(f\"Year Range: {min_year} to {max_year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Process Wildfire Data - Change column names\n",
    "# Rename specific columns\n",
    "wildfire_gdf = wildfire_gdf.rename(columns={\n",
    "    'OBJECTID': 'ID',\n",
    "    'YEAR_': 'Year',\n",
    "    'STATE': 'State',\n",
    "    'AGENCY': 'Agency',\n",
    "    'UNIT_ID': 'Unit ID',\n",
    "    'FIRE_NAME': 'Fire Name',\n",
    "    'INC_NUM': 'Incident Number',\n",
    "    'ALARM_DATE': 'Alarm Date',\n",
    "    'CONT_DATE': 'Containment Date',\n",
    "    'CAUSE': 'Cause',\n",
    "    'C_METHOD': 'Collection Method',\n",
    "    'OBJECTIVE': 'Management Objective',\n",
    "    'GIS_ACRES': 'GIS Acres',\n",
    "    'COMMENTS': 'Comments', \n",
    "    'COMPLEX_NAME': 'Complex Name',\n",
    "    'IRWINID': 'IRWIN ID',\n",
    "    'FIRE_NUM': 'Fire Number',\n",
    "    'COMPLEX_ID': 'Complex ID',\n",
    "    'DECADES':'Decades', \n",
    "    'geometry': 'Geometry'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm column name changes\n",
    "column_names = wildfire_gdf.columns\n",
    "print(\"Column Names:\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only a subset of columns for analysis\n",
    "wildfire_gdf = wildfire_gdf[['ID', 'Year', 'State', 'Agency', 'Unit ID', 'Fire Name',\n",
    "    'Incident Number', 'Alarm Date', 'Containment Date', 'Cause', 'GIS Acres', \n",
    "    'Comments','Complex Name', 'Fire Number', 'Decades','Geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the centroid of each geometry (for polygons)\n",
    "wildfire_gdf['Centroid'] = wildfire_gdf['Geometry'].centroid\n",
    "\n",
    "# Extract latitude and longitude from the centroid\n",
    "wildfire_gdf['Latitude'] = wildfire_gdf['Centroid'].y\n",
    "wildfire_gdf['Longitude'] = wildfire_gdf['Centroid'].x\n",
    "\n",
    "# Convert the geometry column to WKT (Well-Known Text)\n",
    "wildfire_gdf['Geometry'] = wildfire_gdf['Geometry'].apply(lambda x: x.wkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check transformed data\n",
    "print(wildfire_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check all columns\n",
    "column_names = wildfire_gdf.columns\n",
    "print(\"Column Names:\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the sample size\n",
    "num_records = len(wildfire_gdf)\n",
    "print(f\"Number of records: {num_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check fire data type\n",
    "print(wildfire_gdf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform Date columns to datetime\n",
    "wildfire_gdf['Alarm Date'] = pd.to_datetime(wildfire_gdf['Alarm Date'], errors='coerce' )\n",
    "wildfire_gdf['Containment Date'] = pd.to_datetime(wildfire_gdf['Containment Date'], errors='coerce' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how many dates are not valid\n",
    "invalid_dates = wildfire_gdf[wildfire_gdf['Alarm Date'].isna()]\n",
    "print(invalid_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Year Month info from Alarm Date\n",
    "wildfire_gdf['Date'] = wildfire_gdf['Alarm Date'].dt.to_period('M').dt.to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check fire date column\n",
    "print(wildfire_gdf['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only a subset of columns for analysis - remove geometry and centroid data which SQL can't take\n",
    "wildfire_gdf = wildfire_gdf[['ID', 'Year', 'State', 'Agency', 'Unit ID', 'Fire Name',\n",
    "    'Incident Number', 'Alarm Date', 'Containment Date', 'Cause', 'Date',\n",
    "    'GIS Acres', 'Comments', 'Complex Name', 'Fire Number', 'Decades',\n",
    "    'Latitude', 'Longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to a SQL database\n",
    "conn = sqlite3.connect('ca_wildfires.db')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the GeoDataFrame to the SQLite database\n",
    "# The table name will be 'wildfires' in this example\n",
    "wildfire_gdf.to_sql('wildfires', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commit the transaction and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"GeoJSON data with latitude and longitude loaded successfully into the SQLite database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconnect to the SQLite database\n",
    "conn = sqlite3.connect('ca_wildfires.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "# Example query: Get the first 5 records\n",
    "c.execute('SELECT * FROM wildfires LIMIT 5')\n",
    "rows = c.fetchall()\n",
    "\n",
    "# Print the results\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLite wildfire format \n",
    "#  TABLE \"wildfires\" CREATE TABLE \"wildfires\" (\n",
    "# \"ID\" INTEGER,\n",
    "#   \"Year\" REAL,\n",
    "#   \"State\" TEXT,\n",
    "#   \"Agency\" TEXT,\n",
    "#   \"Unit ID\" TEXT,\n",
    "#   \"Fire Name\" TEXT,\n",
    "#   \"Incident Number\" TEXT,\n",
    "#   \"Alarm Date\" TIMESTAMP,\n",
    "#   \"Containment Date\" TIMESTAMP,\n",
    "#   \"Cause\" REAL,\n",
    "#   \"GIS Acres\" REAL,\n",
    "#   \"Comments\" TEXT,\n",
    "#   \"Complex Name\" TEXT,\n",
    "#   \"Fire Number\" TEXT,\n",
    "#   \"Decades\" REAL,\n",
    "#   \"Latitude\" REAL,\n",
    "#   \"Longitude\" REAL\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Rainfall SF Data\n",
    "# Name of the rainfall csv file\n",
    "sf_rain= Path('Resources/sf_rainfall.csv')\n",
    "# The correct encoding must be used to read the CSV in pandas\n",
    "df_sf_rain = pd.read_csv(sf_rain)\n",
    "# Preview of the rain fall dataFrame\n",
    "df_sf_rain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Rainfall LA Data\n",
    "# Name of the rainfall csv file\n",
    "la_rain= Path('Resources/la_rainfall.csv')\n",
    "# The correct encoding must be used to read the CSV in pandas\n",
    "df_la_rain = pd.read_csv(la_rain)\n",
    "# Preview of the rain fall dataFrame\n",
    "df_la_rain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Rainfall San Diego Data\n",
    "# Name of the rainfall csv file\n",
    "sdg_rain= Path('Resources/sdg_rainfall.csv')\n",
    "# The correct encoding must be used to read the CSV in pandas\n",
    "df_sdg_rain = pd.read_csv(sdg_rain)\n",
    "# Preview of the rain fall dataFrame\n",
    "df_sdg_rain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all rainfall data\n",
    "df_rain = pd.concat([df_sf_rain, df_la_rain, df_sdg_rain], axis=0, ignore_index=True)\n",
    "df_rain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform date columns to the correct datetime format\n",
    "df_rain['DATE TIME']=pd.to_datetime(df_rain['DATE TIME'])\n",
    "df_rain['OBS DATE']=pd.to_datetime(df_rain['OBS DATE'])\n",
    "df_rain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with just the year and month in datetime format\n",
    "df_rain['YEAR MONTH'] = df_rain['DATE TIME'].dt.to_period('M').dt.to_timestamp()\n",
    "df_rain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rainfall Station Geo Data\n",
    "# Name of the rainfall station geo csv file\n",
    "rainfall_station = Path('Resources/weather_station_geo.csv')\n",
    "# The correct encoding must be used to read the CSV in pandas\n",
    "df_rain_geo = pd.read_csv(rainfall_station)\n",
    "# Preview of the rain fall dataFrame\n",
    "df_rain_geo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change Rainfall Station Geo Data to be consistent with df_rain data for merge later\n",
    "df_rain_geo.columns = ['STATION NAME', 'STATION_ID', 'ELEV (FEET)', 'LATITUDE', 'LONGITUDE',\n",
    "'COUNTY', 'OPERATOR AGENCY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect mergered data\n",
    "df_precip = pd.merge(df_rain, df_rain_geo, on='STATION_ID', how = 'left')\n",
    "df_precip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform date columns to the correct datetime format\n",
    "df_precip['DATE TIME']=pd.to_datetime(df_precip['DATE TIME'])\n",
    "df_precip['OBS DATE']=pd.to_datetime(df_precip['OBS DATE'])\n",
    "df_precip.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with just the year and month in datetime format\n",
    "df_precip['DATE'] = df_precip['DATE TIME'].dt.to_period('M').dt.to_timestamp()\n",
    "df_precip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform longtitude column from string to float\n",
    "df_precip['LONGITUDE']=df_precip['LONGITUDE'].astype('float')\n",
    "df_precip.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Temperatrue Data\n",
    "sf_temp_path = Path('Resources/avg-temps-sf.csv')\n",
    "la_temp_path = Path('Resources/avg-temps-la.csv')\n",
    "sd_temp_path = Path('Resources/avg-temps-sd.csv')\n",
    "sac_temp_path = Path('Resources/avg-temps-sac.csv')\n",
    "bf_temp_path = Path('Resources/avg-temps-bf.csv')\n",
    "erk_temp_path = Path('Resources/avg-temps-erk.csv')\n",
    "ca_temp_path = Path('Resources/avg-temps-ca.csv')\n",
    "dv_temp_path = Path('Resources/avg-temps-dv.csv')\n",
    "fr_temp_path = Path('Resources/avg-temps-fr.csv')\n",
    "mo_temp_path = Path('Resources/avg-temps-mo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CSVs without unnecessary rows\n",
    "sf_temps_df = pd.read_csv(sf_temp_path, skiprows=3)\n",
    "la_temps_df = pd.read_csv(la_temp_path, skiprows=3)\n",
    "sd_temps_df = pd.read_csv(sd_temp_path, skiprows=3)\n",
    "sac_temps_df = pd.read_csv(sac_temp_path, skiprows=3)\n",
    "bf_temps_df = pd.read_csv(bf_temp_path, skiprows=3)\n",
    "erk_temps_df = pd.read_csv(erk_temp_path, skiprows=3)\n",
    "ca_temps_df = pd.read_csv(ca_temp_path, skiprows=3)\n",
    "dv_temps_df = pd.read_csv(dv_temp_path, skiprows=3)\n",
    "fr_temps_df = pd.read_csv(fr_temp_path, skiprows=3)\n",
    "mo_temps_df = pd.read_csv(mo_temp_path, skiprows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of all city dfs\n",
    "cities_temps = [sf_temps_df, la_temps_df, sd_temps_df, sac_temps_df, bf_temps_df, erk_temps_df, dv_temps_df, fr_temps_df, mo_temps_df, ca_temps_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime format and split year and month into two columns, then drop 'Date' column\n",
    "for city in cities_temps:\n",
    "    city['Date'] = pd.to_datetime(city['Date'], format='%Y%m')\n",
    "    city['Month'] = city['Date'].dt.month\n",
    "    city['Year'] = city['Date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename each 'Value' column to 'Temperatrue'\n",
    "sf_temps_df = sf_temps_df.rename(columns={'Value': 'Temperatrue'})\n",
    "la_temps_df = la_temps_df.rename(columns={'Value': 'Temperatrue'})     \n",
    "sd_temps_df = sd_temps_df.rename(columns={'Value': 'Temperatrue'})     \n",
    "sac_temps_df = sac_temps_df.rename(columns={'Value': 'Temperatrue'})     \n",
    "bf_temps_df = bf_temps_df.rename(columns={'Value': 'Temperatrue'})     \n",
    "erk_temps_df = erk_temps_df.rename(columns={'Value': 'Temperatrue'}) \n",
    "ca_temps_df = ca_temps_df.rename(columns={'Value': 'Temperatrue'})  \n",
    "dv_temps_df = dv_temps_df.rename(columns={'Value': 'Temperatrue'})     \n",
    "fr_temps_df = fr_temps_df.rename(columns={'Value': 'Temperatrue'}) \n",
    "mo_temps_df = mo_temps_df.rename(columns={'Value': 'Temperatrue'})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder each df\n",
    "sf_temps_df = sf_temps_df[['Date', 'Year', 'Month', 'Temperatrue']]\n",
    "la_temps_df = la_temps_df[['Date', 'Year', 'Month', 'Temperatrue']]\n",
    "sd_temps_df = sd_temps_df[['Date', 'Year', 'Month', 'Temperatrue']]\n",
    "sac_temps_df = sac_temps_df[['Date', 'Year', 'Month', 'Temperatrue']]\n",
    "bf_temps_df = bf_temps_df[['Date', 'Year', 'Month', 'Temperatrue']]\n",
    "erk_temps_df = erk_temps_df[['Date', 'Year', 'Month', 'Temperatrue']]\n",
    "ca_temps_df = ca_temps_df[['Date', 'Year', 'Month', 'Temperatrue']]\n",
    "dv_temps_df = dv_temps_df[['Date', 'Year', 'Month', 'Temperatrue']]\n",
    "fr_temps_df = fr_temps_df[['Date', 'Year', 'Month', 'Temperatrue']]\n",
    "mo_temps_df = mo_temps_df[['Date', 'Year', 'Month', 'Temperatrue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check each df\n",
    "print(sf_temps_df)\n",
    "print(la_temps_df)\n",
    "print(sd_temps_df)\n",
    "print(sac_temps_df)\n",
    "print(bf_temps_df)\n",
    "print(erk_temps_df)\n",
    "print(ca_temps_df)\n",
    "print(dv_temps_df)\n",
    "print(fr_temps_df)\n",
    "print(mo_temps_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_temps_df.insert(0, 'City', 'San Francisco')\n",
    "la_temps_df.insert(0, 'City', 'Los Angeles')\n",
    "sd_temps_df.insert(0, 'City', 'San Diego')\n",
    "sac_temps_df.insert(0, 'City', 'Sacramento')\n",
    "bf_temps_df.insert(0, 'City', 'Bakersfield')\n",
    "erk_temps_df.insert(0, 'City', 'Eureka')\n",
    "ca_temps_df.insert(0, 'City', 'California')\n",
    "dv_temps_df.insert(0, 'City', 'Death Valley')\n",
    "fr_temps_df.insert(0, 'City', 'Fresno')\n",
    "mo_temps_df.insert(0, 'City', 'Modesto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to get the coordinates for a city:\n",
    "def get_coordinates(city_name):\n",
    "    geolocator = Nominatim(user_agent=\"wildires\")\n",
    "    location = geolocator.geocode(city_name)\n",
    "    if location:\n",
    "        return location.latitude, location.longitude\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_temps = [sf_temps_df, la_temps_df, sd_temps_df, sac_temps_df, bf_temps_df, erk_temps_df, dv_temps_df, fr_temps_df, mo_temps_df]\n",
    "cities_temps_df = pd.concat(cities_temps, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['San Francisco', 'Los Angeles', 'San Diego', 'Sacramento', 'Bakersfield', 'Eureka', 'California', 'Death Valley', 'Fresno', 'Modesto']\n",
    "cities_coords_df = pd.DataFrame({'City': cities})\n",
    "cities_coords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_coords_df[\"Latitude\"], cities_coords_df[\"Longitude\"] = zip(*cities_coords_df[\"City\"].apply(get_coordinates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_merged_df = pd.merge(cities_temps_df, cities_coords_df, on='City', how='left')\n",
    "cities_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to a SQL database\n",
    "conn = sqlite3.connect('ca_city_temps.db')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the GeoDataFrame to the SQLite database\n",
    "# The table name will be 'wildfires' in this example\n",
    "cities_merged_df.to_sql('city_temps', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commit the transaction and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"GeoJSON data with latitude and longitude loaded successfully into the SQLite database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconnect to the SQLite database\n",
    "conn = sqlite3.connect('ca_city_temps.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "# Example query: Get the first 5 records\n",
    "c.execute('SELECT * FROM city_temps LIMIT 5')\n",
    "rows = c.fetchall()\n",
    "\n",
    "# Print the results\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Fire and Temp SQL Database:\n",
    "# Connect to the first database\n",
    "conn1 = sqlite3.connect('ca_wildfires.db')\n",
    "table1 = pd.read_sql_query(\"SELECT * FROM wildfires\", conn1)\n",
    "\n",
    "# Connect to the second database\n",
    "conn2 = sqlite3.connect('ca_city_temps.db')\n",
    "table2 = pd.read_sql_query(\"SELECT * FROM city_temps\", conn2)\n",
    "\n",
    "# Merge the DataFrames on the 'year_month' column\n",
    "merged_df = pd.merge(table1, table2, on='year_month', how='inner')\n",
    "\n",
    "# Optionally, you can save the merged DataFrame to a new SQL table or a CSV file\n",
    "merged_df.to_sql('merged_table', conn1, if_exists='replace', index=False)\n",
    "\n",
    "# Close the connections\n",
    "conn1.close()\n",
    "conn2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Visualization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Wildfires per Year from 1940 to 2022\n",
    "#Objective: Visualize the trend of wildfire occurrences over the years.\n",
    "#Visualization: Line chart showing the number of wildfires per year.\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('ca_wildfires.db')\n",
    "\n",
    "# Query the database to count the number of wildfires per year\n",
    "query = \"SELECT Year, COUNT(*) as Number_of_Wildfires FROM wildfires WHERE Year >= 1940 AND Year <= 2022 GROUP BY Year ORDER BY Year\"\n",
    "wildfire_counts = pd.read_sql(query, conn)\n",
    "\n",
    "# Plot the number of wildfires per year\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(wildfire_counts['Year'], wildfire_counts['Number_of_Wildfires'], marker='o')\n",
    "plt.title('Number of Wildfires per Year in California')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Wildfires')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Save the plot to a file  in image folder\n",
    "plt.savefig('Images/wildfires_per_year.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total Acres Burned per Year from 1940 to 2022\n",
    "# Objective: Show the total area burned by wildfires each year.\n",
    "# Visualization: Bar chart displaying the total acres burned annually.\n",
    "# Connect to the SQLite database\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('ca_wildfires.db')\n",
    "\n",
    "# Query the database to sum the acres burned per year (focus on 1940 to 2020)\n",
    "query = 'SELECT Year, SUM(\"GIS Acres\") as Total_Acres_Burned FROM wildfires WHERE Year >= 1940 AND Year <= 2022 GROUP BY Year ORDER BY Year'\n",
    "acres_burned = pd.read_sql(query, conn)\n",
    "\n",
    "# Convert Year to integer\n",
    "acres_burned['Year'] = acres_burned['Year'].astype(int)\n",
    "\n",
    "# Plot the total acres burned per year\n",
    "plt.figure(figsize=(16, 8))  # Increase figure size for better readability\n",
    "sns.barplot(x='Year', y='Total_Acres_Burned', data=acres_burned, palette='OrRd')\n",
    "plt.title('Total Acres Burned per Year in California (1940-2020)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Acres Burned')\n",
    "\n",
    "# Rotate all x-axis labels to avoid overlap\n",
    "plt.xticks(rotation=45, fontsize=6)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Save the plot to a file in image folder\n",
    "plt.savefig('Images/total_acres_burned_per_year.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap of Wildfire Occurrences by Month and Year\n",
    "#Objective: Understand seasonal trends in wildfire occurrences.\n",
    "#Visualization: Heatmap showing the number of wildfires by month and year.\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('ca_wildfires.db')\n",
    "\n",
    "# Query the database to get the month and year of each fire\n",
    "query = \"\"\"\n",
    "SELECT strftime('%Y', \"Alarm Date\") as Year, strftime('%m', \"Alarm Date\") as Month, COUNT(*) as Number_of_Wildfires \n",
    "FROM wildfires \n",
    "WHERE CAST(strftime('%Y', \"Alarm Date\") AS INTEGER) >= 1940 \n",
    "AND CAST(strftime('%Y', \"Alarm Date\") AS INTEGER) <= 2022 \n",
    "GROUP BY Year, Month \n",
    "ORDER BY Year, Month\n",
    "\"\"\"\n",
    "monthly_counts = pd.read_sql(query, conn)\n",
    "\n",
    "# Ensure the Month and Year columns are strings\n",
    "monthly_counts['Month'] = monthly_counts['Month'].astype(str)\n",
    "monthly_counts['Year'] = monthly_counts['Year'].astype(str)\n",
    "\n",
    "# Aggregate the data to ensure there are no duplicate Month-Year combinations\n",
    "monthly_counts_agg = monthly_counts.groupby(['Month', 'Year'], as_index=False).sum()\n",
    "\n",
    "# Pivot the data for heatmap\n",
    "monthly_counts_pivot = monthly_counts_agg.pivot(index='Month', columns='Year', values='Number_of_Wildfires')\n",
    "\n",
    "# Replace NaN values with 0 (for months/years with no wildfires)\n",
    "monthly_counts_pivot = monthly_counts_pivot.fillna(0)\n",
    "\n",
    "# Plot a heatmap of wildfire occurrences by month and year\n",
    "plt.figure(figsize=(18, 12))\n",
    "sns.heatmap(monthly_counts_pivot, cmap='Reds', annot=True, annot_kws={\"size\": 6})\n",
    "plt.title('Heatmap of Wildfire Occurrences by Month and Year (1940-2022)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Month')\n",
    "# Rotate all x-axis labels to avoid overlap\n",
    "plt.xticks(rotation=45, fontsize=6)\n",
    "plt.show()\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig('Images/wildfire_heatmap_month_year.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seasonality Analysis (Heatmap)\n",
    "# Group by month and year\n",
    "#Purpose: Analyze the seasonality of wildfires by visualizing the frequency or severity of wildfires by month across multiple years.\n",
    "# Step 1: Connect to the SQLite database\n",
    "conn = sqlite3.connect('ca_wildfires.db')\n",
    "\n",
    "# Query to list all tables\n",
    "# tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", conn)\n",
    "\n",
    "# Load the wildfires data into a pandas DataFrame\n",
    "query = \"SELECT * FROM wildfires\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "df['Alarm Date'] = pd.to_datetime(df['Alarm Date'])\n",
    "df['Month'] = df['Alarm Date'].dt.month\n",
    "seasonality = df.groupby(['Month', df['Alarm Date'].dt.year]).size().unstack()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(seasonality, cmap='YlOrRd', linewidths=.5)\n",
    "plt.title('Wildfire Seasonality (Number of Wildfires)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Month')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig('Images/wildfire_seasonality_fire_frequency', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wildfire Occurrences and Acreage Burned\n",
    "# Load data from the SQLite database\n",
    "conn = sqlite3.connect('ca_wildfires.db')\n",
    "wildfire_data = pd.read_sql('SELECT Year, COUNT(*) as num_fires, SUM(\"GIS Acres\") as total_acres FROM wildfires GROUP BY Year', conn)\n",
    "conn.close()\n",
    "\n",
    "# Plot wildfire occurrences and acreage burned\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax1.set_xlabel('Year')\n",
    "ax1.set_ylabel('Number of Fires', color='tab:orange')\n",
    "ax1.plot(wildfire_data['Year'], wildfire_data['num_fires'], color='tab:orange', label='Number of Fires')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:orange')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Total Acres Burned', color='tab:red')\n",
    "ax2.plot(wildfire_data['Year'], wildfire_data['total_acres'], color='tab:red', label='Total Acres Burned')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title('Wildfire Occurrences and Total Acres Burned in California Over Time')\n",
    "plt.show()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig('Images/wildfire_occurence_acres_over_time.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Interactive Map <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interactive map to show fire location, acres burned and duration of the fire\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('ca_wildfires.db')\n",
    "\n",
    "# Load the wildfires data into a pandas DataFrame\n",
    "query = \"SELECT * FROM wildfires\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Remove rows with NA in the 'Alarm Date' column\n",
    "df = df.dropna(subset=['Alarm Date', 'Containment Date', 'Latitude', 'Longitude', 'GIS Acres'])\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your_table_name' with the name of the table you want to get column headers from\n",
    "table_name = 'wildfires'\n",
    "\n",
    "# Execute a query to get the table schema\n",
    "cursor.execute(f'PRAGMA table_info({table_name})')\n",
    "\n",
    "# Fetch the result\n",
    "columns_info = cursor.fetchall()\n",
    "\n",
    "# Extract column names\n",
    "column_headers = [column[1] for column in columns_info]\n",
    "\n",
    "# Print the column headers\n",
    "print(column_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data loading and processing\n",
    "df['Alarm Date'] = pd.to_datetime(df['Alarm Date'])\n",
    "df['duration_days'] = (pd.to_datetime(df['Containment Date']) - df['Alarm Date']).dt.days\n",
    "\n",
    "# Ensure latitude and longitude are in float format\n",
    "df['Latitude'] = df['Latitude'].astype(float)\n",
    "df['Longitude'] = df['Longitude'].astype(float)\n",
    "\n",
    "# Check for any NA values in critical columns\n",
    "print(\"Checking for NA values:\")\n",
    "print(df[['Latitude', 'Longitude', 'GIS Acres', 'duration_days']].isna().sum())\n",
    "\n",
    "# Check some latitude and longitude values to ensure they are valid\n",
    "print(\"Sample latitude and longitude values:\")\n",
    "print(df[['Latitude', 'Longitude']].head())\n",
    "\n",
    "\n",
    "# Function to determine marker color based on duration\n",
    "def color_producer(duration_days):\n",
    "    if duration_days < 5:\n",
    "        return 'green'\n",
    "    elif 5 <= duration_days < 10:\n",
    "        return 'orange'\n",
    "    else:\n",
    "        return 'red'\n",
    "\n",
    "# Function to create and update the map based on the selected year\n",
    "def update_map(year):\n",
    "    m = folium.Map(location=[36.7783, -119.4179], zoom_start=6)\n",
    "    marker_cluster = MarkerCluster().add_to(m)\n",
    "    \n",
    "    filtered_df = df[df['Alarm Date'].dt.year == year]\n",
    "    print(f\"Number of records for year {year}: {len(filtered_df)}\")  # Debugging output\n",
    "    \n",
    "    for _, row in filtered_df.iterrows():\n",
    "        # print(f\"Placing marker at ({row['Latitude']}, {row['Longitude']}) with radius {row['GIS Acres']}\")\n",
    "        # folium.Marker(\n",
    "        #     location=(row['Latitude'], row['Longitude']),\n",
    "        #     popup=folium.Popup(f\"Acres Burned: {row['GIS Acres']}<br>Duration: {row['duration_days']} days\", max_width=200)\n",
    "        # ).add_to(marker_cluster)\n",
    "        \n",
    "        folium.CircleMarker(\n",
    "            location=(row['Latitude'], row['Longitude']),\n",
    "            radius=max(1, row['GIS Acres'] / 1000),  # Adjust size according to acres burned\n",
    "            color=color_producer(row['duration_days']),\n",
    "            fill=True,\n",
    "            fill_color=color_producer(row['duration_days']),\n",
    "            fill_opacity=0.7,\n",
    "            popup=folium.Popup(f\"Acres Burned: {row['GIS Acres']}<br>Duration: {row['duration_days']} days<br>Fire Name: {row['Fire Name']}\", max_width=200)\n",
    "        ).add_to(marker_cluster)\n",
    "    \n",
    "    return m\n",
    "\n",
    "# Create a dropdown menu for selecting the year\n",
    "years = df['Alarm Date'].dt.year.unique().tolist()\n",
    "year_dropdown = widgets.Dropdown(options=sorted(years), description='Year:')\n",
    "\n",
    "# Display the dropdown and the initial map\n",
    "def display_map(year):\n",
    "    m = update_map(year)\n",
    "    display(m)\n",
    "\n",
    "# Use interactive to link the dropdown to the map update function\n",
    "interactive_map = widgets.interactive(display_map, year=year_dropdown)\n",
    "\n",
    "# Display the interactive widget (dropdown) and the map\n",
    "display(interactive_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap of wildfire duration\n",
    "#Duration of Wildfires Over Time\n",
    "conn = sqlite3.connect('wildfire.db')\n",
    "\n",
    "# Query to list all tables\n",
    "tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Function to filter data by year and update the heatmap\n",
    "def update_heatmap(year):\n",
    "    # Filter data for the selected year\n",
    "    filtered_df = df[df['Alarm Date'].dt.year == year]\n",
    "    \n",
    "    # Prepare the data for the heatmap\n",
    "    heat_data = [[row['Latitude'], row['Longitude']] for index, row in filtered_df.iterrows()]\n",
    "    \n",
    "    # Create a base map\n",
    "    m = folium.Map(location=[36.7783, -119.4179], zoom_start=6)\n",
    "    \n",
    "    # Add the heatmap\n",
    "    HeatMap(heat_data).add_to(m)\n",
    "    \n",
    "    # Display the map\n",
    "    display(m)\n",
    "\n",
    "# Create a dropdown menu for selecting the year\n",
    "years = df['Alarm Date'].dt.year.unique().tolist()\n",
    "year_dropdown = widgets.Dropdown(options=sorted(years), description='Year:')\n",
    "\n",
    "# Use interactive to link the dropdown to the heatmap update function\n",
    "widgets.interactive(update_heatmap, year=year_dropdown)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "nteract": {
   "version": "0.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
